{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1-PyTorch-NLP-TextToTensor.ipynb","provenance":[],"collapsed_sections":["DvrrJ_0K8qCk","bj0_Bcs4_TP8"],"authorship_tag":"ABX9TyMExCx6gCp/NPNH8tzn4FUr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DvrrJ_0K8qCk"},"source":["##Natural Language Tasks\n","\n","There are several NLP tasks that we traditionally try to solve using neural networks:\n","\n","\n","\n","- Text Classification is used when we need to classify text fragment into one of several pre-defined classes. Examples include e-mail spam detection, news categorization, assigning support request to one of the categories, and more.\n","\n","\n"," - Intent Classification is one specific case of text classification, when we want to map input utterance in the conversational AI system into one of the intents that represent the actual meaning of the phrase, or intent of the user.\n","    \n","\n","- Sentiment Analysis is a regression task, where we want to understand the degree of negativity of given piece of text. We may want to label texts in a dataset from the most negative (-1) to most positive ones (+1), and train a model that will output a number of \"positiveness\" of a text.\n","\n","\n"," - Named Entity Recognition (NER) is a task of extracting some entities from text, such as dates, addresses, people names, etc. Together with intent classification, NER is often used in dialog systems to extract parameters from user's utterance.\n","    \n","- A similar task of keyword extraction can be used to find the most meaningful words inside a text, which can then be used as tags.\n","\n","\n","- Text Summarization extracts the most meaningful pieces of text, giving a user a compressed version that contains most of the meaning.\n","    \n","    \n","- Question/Answer is a task of extracting an answer from a piece of text. This model gets text fragment and a question as an input, and needs to find exact place within the text that contains answer. For example, the text \"John is a 22 year old student who loves to use Microsoft Learn\", and the question How old is John should provide us with the answer 22.\n"]},{"cell_type":"markdown","metadata":{"id":"bj0_Bcs4_TP8"},"source":["# Representing text\n","\n","If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors. Computers already represent textual characters as numbers that map to fonts on your screen using encodings such as ASCII or UTF-8.\n","\n","![Image showing diagram mapping a character to an ASCII and binary representation](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/ascii-character-map.png)\n","\n","We understand what each letter **represents**, and how all characters come together to form the words of a sentence. However, computers by themselves do not have such an understanding, and neural network has to learn the meaning during training.\n","\n","Therefore, we can use different approaches when representing text:\n","* **Character-level representation**, when we represent text by treating each character as a number. Given that we have $C$ different characters in our text corpus, the word *Hello* would be represented by $5\\times C$ tensor. Each letter would correspond to a tensor column in one-hot encoding.\n","* **Word-level representation**, in which we create a **vocabulary** of all words in our text, and then represent words using one-hot encoding. This approach is somehow better, because each letter by itself does not have much meaning, and thus by using higher-level semantic concepts - words - we simplify the task for the neural network. However, given large dictionary size, we need to deal with high-dimensional sparse tensors.\n","\n","Let's start by installing some required Python packages we'll use in this module."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGugFBFI8fs_","executionInfo":{"status":"ok","timestamp":1632742107548,"user_tz":-330,"elapsed":4505,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"739bce42-89ef-4881-ea88-63c0c142a823"},"source":["!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (3.8.3)\n","Requirement already satisfied: huggingface==0.0.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 2)) (0.0.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.2.2)\n","Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (3.5)\n","Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 5)) (1.18.5)\n","Requirement already satisfied: opencv-python==4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 6)) (4.5.1.48)\n","Requirement already satisfied: Pillow==7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 7)) (7.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 9)) (1.4.1)\n","Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 10)) (1.8.1)\n","Requirement already satisfied: torchaudio==0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 11)) (0.8.1)\n","Requirement already satisfied: torchinfo==0.0.8 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 12)) (0.0.8)\n","Requirement already satisfied: torchtext==0.9.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (0.9.1)\n","Requirement already satisfied: torchvision==0.9.1 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 14)) (0.9.1)\n","Requirement already satisfied: transformers==4.3.3 in /usr/local/lib/python3.7/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (4.3.3)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (5.2.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (7.1.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (4.62.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (1.0.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 10)) (3.7.4.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (0.0.46)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (3.0.12)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (4.8.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (21.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (0.10.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (1.24.3)\n"]}]},{"cell_type":"markdown","metadata":{"id":"IwYdK-Xu_wOx"},"source":["# Text classification task\n","\n","In this module, we will start with a simple text classification task based on **AG_NEWS** dataset, which is to classify news headlines into one of 4 categories: World, Sports, Business and Sci/Tech. This dataset is built into [`torchtext`](https://github.com/pytorch/text) module, so we can easily access it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZWjodcT8qiR","executionInfo":{"status":"ok","timestamp":1632742120828,"user_tz":-330,"elapsed":3970,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"db9c6924-7917-4f00-fa70-2ad3ce0b56cb"},"source":["import torch\n","import torchtext\n","import os\n","import collections\n","os.makedirs('./data',exist_ok=True)\n","train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n","classes = ['World', 'Sports', 'Business', 'Sci/Tech']"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["train.csv: 29.5MB [00:00, 83.2MB/s]\n","test.csv: 1.86MB [00:00, 39.2MB/s]                  \n"]}]},{"cell_type":"markdown","metadata":{"id":"7cK4QQs4_2dY"},"source":["Here, `train_dataset` and `test_dataset` contain iterators that return pairs of label (number of class) and text respectively, for example:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pczzL17F_5Wh","executionInfo":{"status":"ok","timestamp":1632742124815,"user_tz":-330,"elapsed":357,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"3b793613-29a6-48b2-f519-0eeeb99bef5d"},"source":["next(train_dataset)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3,\n"," \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"wPlnJSeh_9gk"},"source":["So, let's print out the first 10 new headlines from our dataset: "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HB-ONjOnAALy","executionInfo":{"status":"ok","timestamp":1632742129264,"user_tz":-330,"elapsed":402,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"e1debaf9-c07b-40dd-9029-916416671eb4"},"source":["for i,x in zip(range(5),train_dataset):\n","    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n","**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n","**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n","**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n","**Sci/Tech** -> Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n"]}]},{"cell_type":"markdown","metadata":{"id":"TbxyUXwDAFej"},"source":["Because datasets are iterators, if we want to use the data multiple times we need to convert it to list:"]},{"cell_type":"code","metadata":{"id":"NJI6o1pPAFxN","executionInfo":{"status":"ok","timestamp":1632742151394,"user_tz":-330,"elapsed":989,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}}},"source":["train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n","train_dataset = list(train_dataset)\n","test_dataset = list(test_dataset)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIaGyWlKAK0T"},"source":["Now we need to convert text into **numbers** that can be represented as tensors. If we want word-level representation, we need to do two things:\n","* use **tokenizer** to split text into **tokens**\n","* build a **vocabulary** of those tokens."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iDMmesb9ALU3","executionInfo":{"status":"ok","timestamp":1632742206893,"user_tz":-330,"elapsed":382,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"3701004a-9943-4c6e-debc-6c1d37a9d3b5"},"source":["tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n","tokenizer('He said: hello')"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['he', 'said', 'hello']"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"9B6_yzkEAP5m","executionInfo":{"status":"ok","timestamp":1632742213386,"user_tz":-330,"elapsed":4581,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}}},"source":["counter = collections.Counter()\n","for (label, line) in train_dataset:\n","    counter.update(tokenizer(line))\n","vocab = torchtext.vocab.Vocab(counter, min_freq=1)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1zqRvDIAW0O"},"source":["Using vocabulary, we can easily encode out tokenized string into a set of numbers:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITTam9n6AXNm","executionInfo":{"status":"ok","timestamp":1632742236665,"user_tz":-330,"elapsed":359,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"57585797-96b0-4c1c-b488-be80e0756d01"},"source":["print(vocab)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["<torchtext.vocab.Vocab object at 0x7f3f0ac67390>\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yHDalzaVBR8y","executionInfo":{"status":"ok","timestamp":1632742252900,"user_tz":-330,"elapsed":561,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"ad841561-bb0d-4340-9fc8-a3e84e1d5940"},"source":["vocab_size = len(vocab)\n","print(f\"Vocab size if {vocab_size}\")\n","\n","def encode(x):\n","    return [vocab.stoi[s] for s in tokenizer(x)]\n","\n","encode('I love to play with my words')"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size if 95812\n"]},{"output_type":"execute_result","data":{"text/plain":["[283, 2321, 5, 337, 19, 1301, 2357]"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"OIeliSjmCFTW"},"source":["## Bag of Words text representation\n","\n","Because words represent meaning, sometimes we can figure out the meaning of a text by just looking at the individual words, regardless of their order in the sentence. For example, when classifying news, words like *weather*, *snow* are likely to indicate *weather forecast*, while words like *stocks*, *dollar* would count towards *financial news*.\n","\n","**Bag of Words** (BoW) vector representation is the most commonly used traditional vector representation. Each word is linked to a vector index, vector element contains the number of occurrences of a word in a given document.\n","\n","![Image showing how a bag of words vector representation is represented in memory.](https://docs.microsoft.com/en-us/learn/modules/intro-natural-language-processing-pytorch/images/bag-of-words-example.png) \n","\n","> **Note**: You can also think of BoW as a sum of all one-hot-encoded vectors for individual words in the text.\n","\n","Below is an example of how to generate a bag of word representation using the Scikit Learn python library:\""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZqC3ozvBZzv","executionInfo":{"status":"ok","timestamp":1632742486949,"user_tz":-330,"elapsed":1499,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"ccb70284-7ba9-4dc4-cc00-28cc06b1a531"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","corpus = [\n","        'I like hot dogs.',\n","        'The dog ran fast.',\n","        'Its hot outside.',\n","    ]\n","vectorizer.fit_transform(corpus)\n","vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n","\n"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 1, 0, 2, 0, 0, 0, 0, 0]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"b4qif1j7CYt5"},"source":["To compute bag-of-words vector from the vector representation of our AG_NEWS dataset, we can use the following function:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1BKMm7ZCStm","executionInfo":{"status":"ok","timestamp":1632742520356,"user_tz":-330,"elapsed":379,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"9fa812bb-73b5-48ef-c276-9e83940faec9"},"source":["vocab_size = len(vocab)\n","\n","def to_bow(text,bow_vocab_size=vocab_size):\n","    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n","    for i in encode(text):\n","        if i<bow_vocab_size:\n","            res[i] += 1\n","    return res\n","\n","print(to_bow(train_dataset[0][1]))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 0., 2.,  ..., 0., 0., 0.])\n"]}]},{"cell_type":"markdown","metadata":{"id":"JfBzeNaeCfs6"},"source":["> **Note:** Here we are using global `vocab_size` variable to specify default size of the vocabulary. Since often vocabulary size is pretty big, we can limit the size of the vocabulary to most frequent words. Try lowering `vocab_size` value and running the code below, and see how it affects the accuracy. You should expect some accuracy drop, but not dramatic, in lieu of higher performance."]},{"cell_type":"markdown","metadata":{"id":"8CJlD0ulCil4"},"source":["## Training BoW classifier\n","\n","Now that we have learned how to build Bag-of-Words representation of our text, let's train a classifier on top of it. First, we need to convert our dataset for training in such a way, that all positional vector representations are converted to bag-of-words representation. This can be achieved by passing `bowify` function as `collate_fn` parameter to standard torch `DataLoader`:"]},{"cell_type":"code","metadata":{"id":"GVZ1KVIxCbJe","executionInfo":{"status":"ok","timestamp":1632742594049,"user_tz":-330,"elapsed":401,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}}},"source":["from torch.utils.data import DataLoader\n","import numpy as np \n","\n","# this collate function gets list of batch_size tuples, and needs to \n","# return a pair of label-feature tensors for the whole minibatch\n","def bowify(b):\n","    return (\n","            torch.LongTensor([t[0]-1 for t in b]),\n","            torch.stack([to_bow(t[1]) for t in b])\n","    )\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57kBwM1MCvZe"},"source":["Now let's define a simple classifier neural network that contains one linear layer. The size of the input vector equals to `vocab_size`, and output size corresponds to the number of classes (4). Because we are solving classification task, the final activation function is `LogSoftmax()`.\n"]},{"cell_type":"code","metadata":{"id":"aqEauYgYCtIw","executionInfo":{"status":"ok","timestamp":1632742616389,"user_tz":-330,"elapsed":411,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}}},"source":["net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TUqiIpkiC1LT"},"source":["Now we will define standard PyTorch training loop. Because our dataset is quite large, for our teaching purpose we will train only for one epoch, and sometimes even for less than an epoch (specifying the `epoch_size` parameter allows us to limit training). We would also report accumulated training accuracy during training; the frequency of reporting is specified using `report_freq` parameter."]},{"cell_type":"code","metadata":{"id":"0v9BJtVxCylH","executionInfo":{"status":"ok","timestamp":1632742645235,"user_tz":-330,"elapsed":515,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}}},"source":["def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n","    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n","    net.train()\n","    total_loss,acc,count,i = 0,0,0,0\n","    for labels,features in dataloader:\n","        optimizer.zero_grad()\n","        out = net(features)\n","        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss+=loss\n","        _,predicted = torch.max(out,1)\n","        acc+=(predicted==labels).sum()\n","        count+=len(labels)\n","        i+=1\n","        if i%report_freq==0:\n","            print(f\"{count}: acc={acc.item()/count}\")\n","        if epoch_size and count>epoch_size:\n","            break\n","    return total_loss.item()/count, acc.item()/count"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8y9FUHjQC5mT","executionInfo":{"status":"ok","timestamp":1632742680214,"user_tz":-330,"elapsed":416,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"223ced8b-6d19-40e6-c591-dfb9d23b6f8f"},"source":["train_epoch(net,train_loader,epoch_size=1)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.08636809140443802, 0.375)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"I9g1w5tzDEZH"},"source":["## BiGrams, TriGrams and N-Grams\n","\n","One limitation of a bag of words approach is that some words are part of multi word expressions, for example, the word 'hot dog' has a completely different meaning than the words 'hot' and 'dog' in other contexts. If we represent words 'hot` and 'dog' always by the same vectors, it can confuse our model.\n","\n","To address this, **N-gram representations** are often used in methods of document classification, where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. In bigram representation, for example, we will add all word pairs to the vocabulary, in addition to original words. \n","\n","Below is an example of how to generate a bigram bag of word representation using the Scikit Learn:\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6QrOK-H4DCKl","executionInfo":{"status":"ok","timestamp":1632742732452,"user_tz":-330,"elapsed":379,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"552907ba-bbb3-427d-c347-b95011718af2"},"source":["bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n","corpus = [\n","        'I like hot dogs.',\n","        'The dog ran fast.',\n","        'Its hot outside.',\n","    ]\n","bigram_vectorizer.fit_transform(corpus)\n","print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n","bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary:\n"," {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"ko3KDKn9DSlK"},"source":["The main drawback of N-gram approach is that vocabulary size starts to grow extremely fast. In practice, we need to combine N-gram representation with some dimensionality reduction techniques, such as *embeddings*, which we will discuss in the next unit.\n","\n","To use N-gram representation in our **AG News** dataset, we need to build special ngram vocabulary:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXsboLiJDO7g","executionInfo":{"status":"ok","timestamp":1632742775913,"user_tz":-330,"elapsed":14729,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"5c7c4ce6-8894-4c84-b73c-e89aff09f5a6"},"source":["counter = collections.Counter()\n","for (label, line) in train_dataset:\n","    l = tokenizer(line)\n","    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n","    \n","bi_vocab = torchtext.vocab.Vocab(counter, min_freq=1)\n","\n","print(\"Bigram vocabulary length = \",len(bi_vocab))"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Bigram vocabulary length =  1308844\n"]}]},{"cell_type":"markdown","metadata":{"id":"a2BMDDx5DZPg"},"source":["We could then use the same code as above to train the classifier, however, it would be very memory-inefficient. In the next unit, we will train bigram classifier using embeddings.\n","\n","> **Note:** You can only leave those ngrams that occur in the text more than specified number of times. This will make sure that infrequent bigrams will be omitted, and will decrease the dimensionality significantly. To do this, set `min_freq` parameter to a higher value, and observe the length of vocabulary change."]},{"cell_type":"markdown","metadata":{"id":"90P79pdEDjQ_"},"source":["\n","## Term Frequency Inverse Document Frequency TF-IDF\n","\n","In BoW representation, word occurrences are evenly weighted, regardless of the word itself. However, it is clear that frequent words, such as *a*, *in*, etc. are much less important for the classification, than specialized terms. In fact, in most NLP tasks some words are more relevant than others.\n","\n","**TF-IDF** stands for **term frequency–inverse document frequency**. It is a variation of bag of words, where instead of a binary 0/1 value indicating the appearance of a word in a document, a floating-point value is used, which is related to the frequency of word occurrence in the corpus.\n","\n","More formally, the weight $w_{ij}$ of a word $i$ in the document $j$ is defined as:\n","$$\n","w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n","$$\n","where\n","* $tf_{ij}$ is the number of occurrences of $i$ in $j$, i.e. the BoW value we have seen before\n","* $N$ is the number of documents in the collection\n","* $df_i$ is the number of documents containing the word $i$ in the whole collection\n","\n","TF-IDF value $w_{ij}$ increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contains the word, which helps to adjust for the fact that some words appear more frequently than others. For example, if the word appears in *every* document in the collection, $df_i=N$, and $w_{ij}=0$, and those terms would be completely disregarded.\n","\n","You can easily create TF-IDF vectorization of text using Scikit Learn:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lxgOJ7siDRPE","executionInfo":{"status":"ok","timestamp":1632742864955,"user_tz":-330,"elapsed":337,"user":{"displayName":"5017 Antony Prince J","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiZ1EpzxQ0Pzucjqzck4vdLlLNjAmgR8aruHqvYpCA=s64","userId":"15960023355741151365"}},"outputId":"03a64179-3a75-47bf-b974-9dd301b6861f"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(ngram_range=(1,2))\n","vectorizer.fit_transform(corpus)\n","vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n","        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        ]])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"9OSHzZF4DxPV"},"source":["However even though TF-IDF representations provide frequency weight to different words they are unable to represent meaning or order. As the famous linguist J. R. Firth said in 1935, “The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously.”. We will learn in the later units how to capture contextual information from text using language modeling.\n"]},{"cell_type":"code","metadata":{"id":"_3bHI1f6DvSp"},"source":[""],"execution_count":null,"outputs":[]}]}